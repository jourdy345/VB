\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{kotex}
\usepackage{tcolorbox}
\usepackage{setspace} %줄간격 package
%-------------------------------------------------------------------------------------
% Page layout
\addtolength{\textwidth}{1.2in}
\addtolength{\oddsidemargin}{-0.6in}
\addtolength{\textheight}{2.0in}
\addtolength{\topmargin}{-1.0in}
\renewcommand \baselinestretch{1.2}
\parskip = 6pt
% New command
\newcommand {\AND}{\quad\mbox{and}\quad}
\newcommand {\for}{\quad\mbox{ for }}
\newcommand {\half}{{\tfrac 12}}
\newcommand {\bm} [1] {\mbox{\boldmath{$#1$}}}
\newcommand{\bs}{\boldsymbol}
\DeclareMathOperator{\Tr}{Tr}
\begin{document}
\setstretch{1.4} %줄간격 조정
\title{Quantile Regression}
%------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------%
% \setcounter{section}{1}

\section{Introduction to Quantile Regression}
\vspace{0.3cm}
\ \ \ Typically in regression analyses, the standard linear model is as follows:
\begin{equation}
  y_{i} = f\left(\mathbf{x}_{i}\right) + \epsilon_{i},
\end{equation}
where $f\left(x_{i}\right)$ is thought of as the conditional mean of $y_{i}$ given the predictor variables $\mathbf{x}_{i}$. The error term is assumed to be homoskedastic with mean zero which facilitates the inference for the parameters. While the loss function taken for the mean regression is the quadratic error
\begin{equation}
  \min_{\beta} \sum_{i=1}^{n}\left(y_{i}-f\left(\mathbf{x}_{i}\right)\right)^{2},
\end{equation}
the least squares method simply will not cut it for quantile regression which calls for another type of a loss function. We will call this the \emph{check function} following the literature of Kneib and is expressed as
\begin{equation}
  \rho_{p}\left(x\right) = x\left(p-I\left(x < 0\right)\right),
\end{equation}
where $p$ is the conditional quantile of interest that is being modeled.
Equivalent representations exist:
\begin{align*}
  \rho_{p}\left(x\right) &= x\left(pI\left(x > 0\right) - \left(1-p\right)I\left(x<0\right)\right)\\
  &= \frac{1}{2}\left(\left|x\right| +\left(2p-1\right)x\right).
\end{align*}
Now the loss function transforms to
\begin{equation}
  \min_{\beta}\sum_{i=1}^{n}\rho_{p}\left(y_{i}-f\left(\mathbf{x}\right)\right).
\end{equation}
\subsection{Asymmetric Laplace distribution}
Although the frequentist design of quantile regression does not allow for a likelihood function for the response variables since they directly model the loss function, Bayesian approach does not stand a chance without it as it is an indispensable machinery for the tools of Bayesian inference. Therefore, we introduce a density that we can play with whose maximization is shown to be equivalent to the minimization of the frequentist loss function. Somehow Bayesian methodology is inextricably dependent upon the frequentist formulation. What a shame. Anyway, the density is called the \emph{asymmetric Laplace distribution} whose density is as follows:
\begin{equation}
  f_{X}\left(x\,|\,p\right) = p\left(1-p\right)\exp\left(-\rho_{p}\left(x\right)\right),
\end{equation}
where, as before, $0<p<1$, which becomes the quantile of interest, becomes the parameter and $\rho$ is the check function. Setting $p = 1/2$ puts back the asymmetric version to the symmetric Laplace distribution. It can also embrace both location and scale parameters like the Gaussian distribution:
\begin{equation}
  f_{X}\left(x\,|\,p,\mu,\sigma\right) = \frac{p\left(1-p\right)}{\sigma}\exp\left(-\rho_{p}\left(\frac{x-\mu}{\sigma}\right)\right)\text{ if $X \sim \mathrm{ALD}\left(\mu,\sigma,p \right)$}.
\end{equation}
\subsection{Alternative Representations for ALD}
There arise situations where, for instance, the mixing speed of an MCMC algorithm is too slow and another representation is needed to accelerate inference. For such circumstances, we provide useful alternatives to the vanilla density function of ALD. \par
Let $U \sim \mathrm{Exp}\left(\sigma^{-1}\right)$ and $Z \sim \mathcal{N}\left(0,1\right)$ be independeant random variables. (Exponential distribution is rate-parameterized.) Then $Y \sim \mathrm{ALD}\left(\mu, \sigma, p\right)$ can be represented by
\begin{equation}
  Y \overset{d}{\sim} \mu + \nu_{p}U + \tau_{p}\sqrt{\sigma U}Z,
\end{equation}
where $\nu_{p}=\left(p\left(1-p\right)\right)^{-1}\left(1-2p\right)$ and $\tau_{p}^{2} = 2\left(p\left(1-p\right)\right)^{-1}$. \par
(7) allows a much handier mixture representation of ALD.
\begin{align*}
  Y\,|\,U=u &\sim \mathcal{N}\left(\mu + \nu_{p}u, \tau_{p}^{2}\sigma u\right),\\
  U &\sim \mathrm{Exp}\left(\sigma^{-1}\right).
\end{align*}
It naturally follows that the conditional distribution of $U$ given $Y$, is $U\,|\,Y=y \sim \mathrm{GIG}\left(2^{-1}, \delta, \gamma\right)$ where $\delta =\tau_{p}^{-1}\sigma^{-1/2}\left|y-\mu\right|$ and $\gamma =\sigma^{-1/2}\left(2+\tau_{p}^{-2}\nu_{p}^{2}\right)^{1/2}=2^{-1}\sigma^{-1/2}\tau_{p}$. $\mathrm{GIG}\left(\omega, a, b\right)$ denotes the \emph{generalized inverse Gaussian distribution} with pdf
\begin{equation}
  f\left(x\,|\,\omega, a, b\right) = \frac{\left(b/a\right)^{\omega}}{2K_{\omega}\left(ab\right)}x^{\omega-1}\exp\left(-\frac{1}{2}\left(a^{2}x^{-1}+b^{2}x\right)\right), \quad x>0,\; \omega\in \mathbb{R}, \; a,b>0,
\end{equation}
where $K_{\omega}\left(\cdot\right)$ is a modified Bessel function of the third kind. The moments of $X$ are given by
\begin{equation}
  \mathrm{E}\left(X^{k}\right) = \left(\frac{a}{b}\right)^{k}\frac{K_{\omega+k}\left(ab\right)}{K_{\omega}\left(ab\right)},\quad k\in\mathbb{R}.
\end{equation}
For the time being, keep in mind some basic properties of the Bessel function of the third kind:
\begin{itemize}
  \item $K_{\omega}\left(x\right) = K_{-\omega}\left(x\right)$
  \item $K_{\omega+1}\left(x\right) = \frac{2\omega}{x}K_{\omega}\left(x\right) + K_{\omega-1}\left(x\right)$
  \item $K_{^{r+1/2}}\left(x\right)=\sqrt{\dfrac{\pi}{2x}}\exp\left(-x\right)\displaystyle \sum_{k=1}^{r}\dfrac{\left(r+k\right)!\left(2x\right)^{-k}}{\left(r-k\right)!k!}, \quad \forall r \in \mathbb{Z}_{+}\cup\left\{0\right\}$
  \item $K_{1/2}\left(x\right) = \sqrt{\dfrac{\pi}{2x}}\exp\left(-x\right)$
\end{itemize}
\section{Quantile Regression}
  As we always do, if we know how to set up the likelihood function and prior distributions, then we can come up with some sort of inference mechanism whether be it MCMC or VB. Although it is possible to keep the ALD density within the likelihood function, it is hard to deal with indicator functions hidden inside the check function. This is where the mixture representation comes in handy. We will cosine-transform the covariate and impose the same priors as in Lenk and Choi(2015).
\subsection{Likelihood}
Using the mixture representation,
\begin{align}
  L\left(u_{1}, \ldots , u_{n}, \bs{\beta}, \bs{\theta}_{J}, \gamma, \tau^{2}|\mathbf{y}\right) &\propto \prod_{i=1}^{n}u_{i}^{-1/2}\;\cdot \exp\left(-\sum_{i=1}^{n}\frac{\left(y_{i}-\bs{w}_{i}^{\top}\bs{\beta}-\bs{\varphi}_{i}^{\top}\bs{\theta}_{J}-\nu_{p}u_{i}\right)^{2}}{2\tau_{p}^{2}u_{i}} \right)\\
  &\quad \prod_{i=1}^{n}\exp\left(-u_{i}\right)\;\cdot\exp\left(-\frac{1}{2}\left(\bs{\beta}-\bs{\mu}_{\beta}^{0}\right)^{\top}{\bs{\Sigma}_{\beta}^{0}}^{-1}\left(\bs{\beta}-\bs{\mu}_{\beta}^{0}\right)\right)\\
  &\quad \prod_{j=1}^{J}\left(\tau^{2}e^{-j\gamma}\right)^{-1/2}\exp\left(-\frac{e^{j\gamma}}{2\tau^{2}}\theta_{j}^{2}\right)\;\cdot \exp\left(-\omega_{0}\gamma\right)\\
  &\quad {\tau^{2}}^{-\left(A+1\right)}\exp\left(-B/\tau^{2}\right).
\end{align}
As can be seen, the specifications are as follows separately:
\begin{itemize}
  \item $y_{i}\,|\,u_{i}, \bs{\beta},\bs{\theta}_{J} \sim \mathcal{N}\left(\bs{w}_{i}^{\top}\bs{\beta}+\bs{\varphi}_{i}^{\top}\bs{\theta}_{J}+\nu_{p}u_{i}, \tau_{p}^{2}u_{i}\right)$
  \item $u_{i}\sim \mathrm{Exp}\left(1\right)$
  \item $\bs{\beta} \sim \mathcal{N}\left(\bs{\mu}_{\beta}^{0}, \bs{\Sigma}_{\beta}^{0}\right)$
  \item $\bs{\theta}_{J}\sim \mathcal{N}\left(0, \tau^{2}\exp\left(-j\gamma \right)\right)$
  \item $\gamma \sim \mathrm{Exp}\left(\omega_{0}\right)$
  \item $\tau^{2} \sim \mathrm{InvGam}\left(A,B\right)$
\end{itemize}
\subsection{MCMC scheme}
Every parameter excluding $\gamma$ is conditionally conjugate enough to achieve a Gibbs sampler. The inference for $\gamma$ requires MH updates.
\begin{itemize}
  \item $u_{i}\,|\, \bs{\beta}, \bs{\theta}_{J}, y_{i} \sim \mathrm{GIG}\left(2^{-1}, \tau_{p}^{-1}\left|y_{i}-\bs{w}_{i}^{\top}\bs{\beta}-\bs{\varphi}_{i}^{\top}\bs{\theta}_{J}\right|, 2^{-1}\tau_{p}\right)$
  \item $\bs{\beta}\,|\, \left\{u_{i}\right\}_{i=1}^{n}, \bs{\theta}_{J} \sim \mathcal{N}\left(\bs{\mu}_{\beta}^{\text{pos}}, \bs{\Sigma}_{\beta}^{\text{pos}}\right)$ where
  \begin{itemize}
    \item $\bs{\Sigma}_{\beta}^{\text{pos}} = \left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\dfrac{1}{u_{i}}\bs{w}_{i}\bs{w}_{i}^{\top} + {\bs{\Sigma}_{\beta}^{0}}^{-1} \right)^{-1}$
    \item $\bs{\mu}_{\beta}^{\text{pos}} = \bs{\Sigma}_{\beta}^{\text{pos}}\left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\dfrac{\bs{w}_{i}\left(y_{i}-\bs{\varphi}_{i}^{\top}\bs{\theta}_{J}-\nu_{p}u_{i}\right)}{u_{i}}+{\bs{\Sigma}_{\beta}^{0}}^{-1}\bs{\mu}_{\beta}^{0} \right) $
  \end{itemize}
  \item $\bs{\theta}_{J}\,|\,\left\{u_{i}\right\}_{i=1}^{n}, \gamma, \bs{\beta}, \mathbf{y} \sim \mathcal{N}\left(\bs{\mu}_{\theta}^{\text{pos}}, \bs{\Sigma}_{\theta}^{\text{pos}}\right)$ where
  \begin{itemize}
    \item $\bs{\Sigma}_{\theta}^{\text{pos}} = \left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\dfrac{1}{u_{i}}\bs{\varphi}_{i}\bs{\varphi}_{i}^{\top} + \bs{E}\right)^{-1}$, where $\bs{E}=\dfrac{1}{\tau^{2}}\mathrm{diag}\left(e^{\gamma}, e^{2\gamma}, \ldots , e^{J\gamma}\right)$
    \item $\bs{\mu}_{\theta}^{\text{pos}}= \bs{\Sigma}_{\theta}^{\text{pos}}\left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\dfrac{1}{u_{i}}\bs{\varphi}_{i}\left(y_{i}-\bs{w}_{i}^{\top}\bs{\beta}-\nu_{p}u_{i}\right)\right)$
  \end{itemize}
  \item $\tau^{2}\,|\,\bs{\theta}_{J}, \gamma \sim \mathrm{InvGam}\left(A+ \dfrac{J}{2}, \dfrac{1}{2}\bs{\theta}_{J}^{\top}\bs{E}^{*}\bs{\theta}_{J} + B\right), \quad \bs{E}^{*}=\mathrm{diag}\left(e^{\gamma}, e^{2\gamma}, \ldots , e^{J\gamma}\right)$
  \item $p\left(\gamma\,|\,\tau^{2},\bs{\theta}_{J}\right) \propto \exp\left(\left(\dfrac{J\left(J+1\right)}{4}-\omega_{0}\right)\gamma -\dfrac{1}{2\tau^{2}}\displaystyle \sum_{j=1}^{J}e^{j\gamma}\theta_{j}^{2} \right)$
\end{itemize}
The full conditional of $\gamma$ resembles the Gompertz density with canonical parameters
\begin{equation}
  f\left(x\,|\,\eta, b\right) = b\eta e^{\eta}e^{bx}\exp\left(-\eta e^{bx}\right)\quad \text{for $x\geq 0$, $\eta, b > 0$}
\end{equation}
but is impossible to separate out the parameters; hence, we construct an MH chain with a standard exponential proposal. If we choose the standard exponential distribution to be our proposal, then the algorithm is as follows:
\begin{enumerate}
  \item Generate $\gamma_{\text{cand}} \sim \mathrm{Exp}\left(1\right)$.
  \item Take $\gamma^{\left(t+1\right)} = \begin{cases}\gamma_{\text{cand}}, & \text{with $p = 1 \wedge \dfrac{f\left(\gamma_{\text{cand}}\right)g\left(\gamma^{\left(t\right)}\right)}{f\left(\gamma^{\left(t\right)}\right)g\left(\gamma_{\text{cand}}\right)}\,(=\rho)$}\\ \gamma^{\left(t\right)} & \text{otherwise}  \end{cases}$,   where
  \begin{itemize}
    \item $f\left(\gamma\right) = \exp\left(\left(\dfrac{J\left(J+1\right)}{4}-\omega_{0}\right)\gamma - \dfrac{1}{2\tau^{2}}\displaystyle \sum_{j=1}^{J}e^{j\gamma}\theta_{j}^{2} \right) $
    \item $g\left(\gamma\right) = \exp\left(-\gamma\right)$.
    \item $\wedge$ is `choose the minimum' operator.
  \end{itemize}
\end{enumerate}
 Rearranging the elements,
 \begin{equation}
  \rho = \exp\left(\left(\frac{J\left(J+1\right)}{4}-\omega_{0}\right)\left(\gamma_{\text{cand}}-\gamma^{\left(t\right)}\right)-\frac{1}{2\tau^{2}}\sum_{j=1}^{J}\left(e^{j\gamma_{\text{cand}}}-e^{j\gamma^{\left(t\right)}}\right)\theta_{j}^{2}-\gamma^{\left(t\right)}+\gamma_{\text{cand}} \right)
 \end{equation}
\subsection{Variational Inference}
  Variational inference is an alternative to MCMC which aims for speed improvement at the expense of performance. There is a vast literature on the variational methods so we do not elaborate more on this topic. For those who are interested should be referred to Wand \& Ormerod (2010).
\begin{itemize}
  \item $q\left(u_{i}\right) = \mathrm{GIG}\left(\dfrac{1}{2}, a_{q}, b_{q}\right)$ where
  \begin{itemize}
    \item $a_{q}^{2}= \dfrac{1}{\tau_{p}^{2}}\left(\left(y_{i}-\bs{w}_{i}^{\top}\bs{\mu}_{\beta}^{q}-\bs{\varphi}_{i}^{\top}\bs{\mu}_{\theta}^{q}\right)^{2}+\Tr\left(\bs{w}_{i}\bs{w}_{i}^{\top}\bs{\Sigma}_{\beta}^{q}\right)+\Tr\left(\bs{\varphi}_{i}\bs{\varphi}_{i}^{\top}\bs{\Sigma}_{\theta}^{q}\right) \right) $
    \item $b_{q}^{2}=2 + \dfrac{\nu_{p}^{2}}{\tau_{p}^{2}}$
  \end{itemize}
  \item $q\left(\bs{\beta}\right)=\mathcal{N}\left(\bs{\mu}_{\beta}^{q},\bs{\Sigma}_{\beta}^{q}\right)$
  \begin{itemize}
    \item $\bs{\Sigma}_{\beta}^{q}= \left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\dfrac{1}{u_{i}}\bs{w}_{i}\bs{w}_{i}^{\top}+{\bs{\Sigma}_{\beta}^{0}}^{-1}\right)^{-1} $
    \item $\bs{\mu}_{\beta}^{q} = \bs{\Sigma}_{\beta}^{q}\left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\bs{w}_{i}\left(\mathrm{E}\left(\dfrac{1}{u_{i}}\right)\left(y_{i}-\bs{\varphi_{i}^{\top}}\bs{\mu}_{\theta}^{q}\right)-\nu_{p}\right)+{\bs{\Sigma}_{\beta}^{0}}^{-1}\bs{\mu}_{\beta}^{0} \right) $
  \end{itemize}
  \item $q\left(\bs{\theta}_{J}\right)=\mathcal{N}\left(\bs{\mu}_{\theta}^{q},\bs{\Sigma}_{\theta}^{q}\right)$
  \begin{itemize}
    \item $\bs{\Sigma}_{\theta}^{q}=\left(\dfrac{1}{\tau_{p}^{2}}\displaystyle \sum_{i=1}^{n}\mathrm{E}\left(\dfrac{1}{u_{i}}\right)\bs{\varphi}_{i}\bs{\varphi}_{i}^{\top}+\bs{E} \right)^{-1}$ where $\bs{E}=\mathrm{E}\left(\dfrac{1}{\tau^{2}}\right)\mathrm{diag}\left(\mathrm{E}\left(e^{\gamma}\right), \mathrm{E}\left(e^{2\gamma}\right),\ldots, \mathrm{E}\left(e^{J\gamma}\right) \right) $
    \item $\bs{\mu}_{\theta}^{q} = \bs{\Sigma}_{\theta}^{q}\left(\dfrac{1}{\tau_{p}^{2}}\displaystyle\sum_{i=1}^{n}\bs{\varphi}_{i}\left(\mathrm{E}\left(\dfrac{1}{u_{i}}\right)\left(y_{i}-\bs{w}_{i}^{\top}\bs{\mu}_{\beta}^{q}\right)-\nu_{p}\right)\right)$
  \end{itemize}
  \item $q\left(\tau^{2}\right) = \mathrm{InvGam}\left(A_{q},B_{q}\right)$
  \begin{itemize}
    \item $A_{q}=\dfrac{J}{2}+A$
    \item $B_{q}=\dfrac{1}{2}\left({\bs{\mu}_{\theta}^{q}}^{\top}\bs{E}^{*}\bs{\mu}_{\theta}^{q}+\Tr\left(\bs{E}^{*}\bs{\Sigma}_{\theta}^{q}\right)\right)+B$ where $\bs{E}^{*}=\mathrm{diag}\left(\mathrm{E}\left(e^{\gamma}\right),\ldots,\mathrm{E}\left(e^{J\gamma}\right)\right)$.
  \end{itemize}
\end{itemize}
\end{document}
Single index
Seongbae Jeon, 2015.2.4